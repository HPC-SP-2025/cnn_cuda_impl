\section{Introduction}

Modern deep learning frameworks like PyTorch~\cite{pytorch_tutorial, techtarget_pytorch} and TensorFlow~\cite{tensorflow_wiki} offer powerful abstractions for building, training, and deploying neural networks with minimal effort. However, these abstractions often obscure the underlying mechanisms of model execution, memory management, and backpropagation, especially when executed on GPUs~\cite{builtin_tf_vs_pytorch}. To bridge this gap and gain a deeper understanding of neural network internals, our project aims to replicate the core functionalities of PyTorch's \texttt{torch.nn} module~\cite{pytorch_nn_example} from scratch using C++, CUDA, and OpenMP.

The objective of this project is to implement a minimal neural network framework capable of training a simple convolutional neural network (CNN) on the MNIST dataset. The framework supports key operations such as forward propagation, backward propagation, and parameter updates. In this report, we present the architecture and implementation details of our framework, discuss the training workflow, and highlight the challenges and learnings encountered during development.


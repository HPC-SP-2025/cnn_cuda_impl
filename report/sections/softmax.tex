\section{Softmax}

The Softmax layer applies the Softmax function on a vector of \( K \) numbers, converting the vector into a probability distribution with \( K \) possible outcomes. This layer is typically introduced at the end of a classification network to convert the output of the network into a probability distribution over classes~\cite{bishop2006pattern, goodfellow2016deep}.

\subsection{Forward Pass}

The Softmax function is applied element-wise to the input vector. The computation is defined as:
\[
    y_i = \text{softmax}(x_i) = \frac{e^{x_i - x_m}}{\sum_{j=1}^K e^{x_j - x_m}}
\]
where:
\begin{itemize}
    \item \( x \in \mathbb{R}^K \): Input vector, where \( K \) is the number of classes.
    \item \( x_m = \max_i x_i \): Maximum value in \( x \), used for numerical stability.
    \item \( y \in (0, 1)^K \): Output probability vector.
\end{itemize}

\textbf{CPU Implementation:}  
The CPU implementation uses nested loops:
\begin{itemize}
    \item An outer loop iterates over batch elements.
    \item The inner loop computes \( x_m \), exponentiates \( x_i - x_m \), and accumulates the sum.
    \item A normalization loop divides each exponentiated value by the sum.
\end{itemize}
Subtracting \( x_m \) ensures numerical stability by bounding the exponent values~\cite{goodfellow2016deep}.

\textbf{GPU Implementation:}  
On the GPU, a custom CUDA kernel (the \textit{Softmax forward kernel}) parallelizes computation across batch elements~\cite{nvidia_cudnn}:
\begin{itemize}
    \item Each thread processes one batch element (a vector \( x \)).
    \item The kernel computes \( x_m \), exponentiates \( x_i - x_m \), and normalizes.
\end{itemize}
This design leverages GPU parallelism while avoiding unstable exponentiation~\cite{digitalocean_gpu_opt}.

\subsection{Backward Pass}

The backward pass of the Softmax layer works in tandem with the loss layer. When combined with cross-entropy loss, the gradient simplifies to:
\[
    \nabla_x = \nabla_y
\]
where:
\begin{itemize}
    \item \( \nabla_y \in \mathbb{R}^K \): Upstream gradient from the loss layer.
    \item \( \nabla_x \in \mathbb{R}^K \): Gradient with respect to the input.
\end{itemize}
This simplification is a well-known result in training classification networks~\cite{goodfellow2016deep}.

\textbf{CPU Implementation:}  
The backward pass copies \( \nabla_y \) to \( \nabla_x \) using nested loops.

\textbf{GPU Implementation:}  
A custom CUDA kernel (the \textit{Softmax backward kernel}) assigns one thread per batch element to copy gradients~\cite{nvidia_cudnn}.

\subsection{Memory Management}

The layer maintains input/output buffers for forward and backward passes. Device memory is allocated via \texttt{setDevice()} when switching to GPU mode~\cite{aws_gpu_performance}. No parameters are stored, as Softmax is parameter-free.

\subsection{Design Considerations}

The implementation parallelizes across batch elements rather than individual vector elements, as:
\begin{itemize}
    \item Class counts are small (e.g., 10 for MNIST).
    \item Batch sizes can reach \( 2^{13} \), benefiting from coarse-grained parallelism~\cite{neptune_gpu_optimization}.
\end{itemize}
